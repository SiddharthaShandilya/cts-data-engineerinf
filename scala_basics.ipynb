{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c887c4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://SIddhartha:4040\n",
       "SparkContext available as 'sc' (version = 3.0.3, master = local[*], app id = local-1653472681972)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Spark context...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkConf\r\n",
       "import org.apache.spark.sql.SparkSession\r\n",
       "import org.apache.spark.sql.SaveMode\r\n",
       "import org.apache.hadoop.conf.Configuration\r\n",
       "import org.apache.hadoop.fs.FileSystem\r\n",
       "import org.apache.hadoop.fs.Path\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.SaveMode\n",
    "\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "import org.apache.hadoop.fs.FileSystem\n",
    "import org.apache.hadoop.fs.Path\n",
    "\n",
    "println(\"Initializing Spark context...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e5df8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+----------+-----+------+---+-----+\n",
      "|employee_id|      employee_name|department|state|salary|age|bonus|\n",
      "+-----------+-------------------+----------+-----+------+---+-----+\n",
      "|       1000|          Nitz Leif| Marketing|   CA|  6131| 26|  543|\n",
      "|       1001|    Melissia Dedman|   Finance|   AK|  4027| 43| 1290|\n",
      "|       1002|  Rudolph Barringer|        HR|   LA|  3122| 43| 1445|\n",
      "|       1003|        Tamra Amber|  Accounts|   AK|  5717| 47| 1291|\n",
      "|       1004|        Mullan Nitz|Purchasing|   CA|  5685| 34| 1394|\n",
      "|       1005|      Zollner Karie|  Accounts|   CA|  2843| 27| 1078|\n",
      "|       1006|Kaczorowski Zollner|     Sales|   CA|  7201| 21| 1834|\n",
      "|       1007|      Nakano Locust| Marketing|   LA|  3444| 23| 1823|\n",
      "|       1008|  Recalde Kensinger|  Accounts|   LA|  3704| 48| 1330|\n",
      "|       1009|        Imai Hallie|  Accounts|   AK|  5061| 38| 1557|\n",
      "|       1010|    Debroah Gallman|  Accounts|   NY|  9308| 35|  817|\n",
      "|       1011|   Barringer Escoto|Purchasing|   WA|  1685| 49| 1706|\n",
      "|       1012|      Soules Coogan|  Accounts|   AK|  8330| 43| 1914|\n",
      "|       1013|      Luisa Suzanne|  Accounts|   CA|  1151| 37| 1095|\n",
      "|       1014|      Marvis Cobian|Purchasing|   NY|  5061| 41| 1765|\n",
      "|       1015|   Cobian Kensinger|     Sales|   LA|  1983| 21|  632|\n",
      "|       1016|      Gilma Margret| Marketing|   CA|  2919| 45| 1762|\n",
      "|       1017| Ellingsworth Ilana|  Accounts|   WA|  9614| 26| 1964|\n",
      "|       1018| Vankirk Jacquelyne|Purchasing|   NY|  8636| 47| 1192|\n",
      "|       1019|    Zollner Juliana|        HR|   NY|  9739| 30| 1119|\n",
      "+-----------+-------------------+----------+-----+------+---+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "root\n",
      " |-- employee_id: integer (nullable = true)\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- bonus: integer (nullable = true)\n",
      "\n",
      "()"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [employee_id: int, employee_name: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.option(\"header\",true).option(\"inferSchema\",true).csv(\"Data/OfficeDataProject.csv\")\n",
    "df.show()\n",
    "\n",
    "print(\"\\n\"+\"--\"*30+\"\\n\")\n",
    "\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2e23b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f9b857a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(using above functions,123.0)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "total_salary: (state: String, salary: Int, bonus: Int)Double\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    " \n",
    "//UDF -> User Defined Functions(Custom Requirements)\n",
    "//#Simple UDF\n",
    "//#increase salary for each employee by state\n",
    "//#if state is NY -> salary by 10% and bonus by 5%\n",
    "//#if state is CA -> salary by 12% and bonus by 3%\n",
    "def total_salary(state:String ,salary: Int,bonus: Int): Double = {\n",
    "    var demo = 1.0\n",
    "    if (state == \"NY\"){\n",
    "        demo = (salary * 0.10)\n",
    "        demo =  demo + bonus * 0.05\n",
    "        }\n",
    "    else if (state == \"CA\"){\n",
    "        demo = (salary * 0.12)\n",
    "        demo =  demo + bonus * 0.03\n",
    "        }\n",
    "    return demo\n",
    " \n",
    " }\n",
    "\n",
    "print(\"using above functions\",total_salary(\"CA\",1000,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d30f3cd",
   "metadata": {},
   "source": [
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2207aab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+----------+-----+------+---+-----+------------------+\n",
      "|employee_id|      employee_name|department|state|salary|age|bonus|         increment|\n",
      "+-----------+-------------------+----------+-----+------+---+-----+------------------+\n",
      "|       1000|          Nitz Leif| Marketing|   CA|  6131| 26|  543|            752.01|\n",
      "|       1001|    Melissia Dedman|   Finance|   AK|  4027| 43| 1290|               1.0|\n",
      "|       1002|  Rudolph Barringer|        HR|   LA|  3122| 43| 1445|               1.0|\n",
      "|       1003|        Tamra Amber|  Accounts|   AK|  5717| 47| 1291|               1.0|\n",
      "|       1004|        Mullan Nitz|Purchasing|   CA|  5685| 34| 1394|            724.02|\n",
      "|       1005|      Zollner Karie|  Accounts|   CA|  2843| 27| 1078|373.49999999999994|\n",
      "|       1006|Kaczorowski Zollner|     Sales|   CA|  7201| 21| 1834|            919.14|\n",
      "|       1007|      Nakano Locust| Marketing|   LA|  3444| 23| 1823|               1.0|\n",
      "|       1008|  Recalde Kensinger|  Accounts|   LA|  3704| 48| 1330|               1.0|\n",
      "|       1009|        Imai Hallie|  Accounts|   AK|  5061| 38| 1557|               1.0|\n",
      "|       1010|    Debroah Gallman|  Accounts|   NY|  9308| 35|  817| 971.6500000000001|\n",
      "|       1011|   Barringer Escoto|Purchasing|   WA|  1685| 49| 1706|               1.0|\n",
      "|       1012|      Soules Coogan|  Accounts|   AK|  8330| 43| 1914|               1.0|\n",
      "|       1013|      Luisa Suzanne|  Accounts|   CA|  1151| 37| 1095|            170.97|\n",
      "|       1014|      Marvis Cobian|Purchasing|   NY|  5061| 41| 1765|            594.35|\n",
      "|       1015|   Cobian Kensinger|     Sales|   LA|  1983| 21|  632|               1.0|\n",
      "|       1016|      Gilma Margret| Marketing|   CA|  2919| 45| 1762|            403.14|\n",
      "|       1017| Ellingsworth Ilana|  Accounts|   WA|  9614| 26| 1964|               1.0|\n",
      "|       1018| Vankirk Jacquelyne|Purchasing|   NY|  8636| 47| 1192|             923.2|\n",
      "|       1019|    Zollner Juliana|        HR|   NY|  9739| 30| 1119|1029.8500000000001|\n",
      "+-----------+-------------------+----------+-----+------+---+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sidudf: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$3405/720466668@4510ffff,DoubleType,List(Some(class[value[0]: string]), Some(class[value[0]: int]), Some(class[value[0]: int])),None,false,true)\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "var sidudf = udf((x: String,y:Int, z:Int) => total_salary(x,y,z)) \n",
    "// \n",
    "df.withColumn(\"increment\",sidudf(df(\"state\"),df(\"salary\"),df(\"bonus\"))).show();\n",
    " \n",
    "//\n",
    "//  val featureHashUDF = udf((dim: String, value: String) => WalLogAgg.toFeatureHash(dim, value))\n",
    "//\n",
    "//    filteredDict.withColumn(\"featureHash\", featureHashUDF(col(\"dim\"), col(\"value\")))\n",
    " //     .select(\"featureHash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb9bcace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: df.type = [employee_id: int, employee_name: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc29c21",
   "metadata": {},
   "source": [
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "581f991b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------+------+------+-----+--------------------+\n",
      "|age|gender|            name|course|  roll|marks|               email|\n",
      "+---+------+----------------+------+------+-----+--------------------+\n",
      "| 28|Female| Hubert Oliveras|    DB|  2984|   59|Annika Hoffman_Na...|\n",
      "| 29|Female|Toshiko Hillyard| Cloud| 12899|   62|Margene Moores_Ma...|\n",
      "| 28|  Male|  Celeste Lollis|    PF| 21267|   45|Jeannetta Golden_...|\n",
      "| 29|Female|    Elenore Choy|    DB| 32877|   29|Billi Clore_Mitzi...|\n",
      "| 28|  Male|  Sheryll Towler|   DSA| 41487|   41|Claude Panos_Judi...|\n",
      "| 28|  Male|  Margene Moores|   MVC| 52771|   32|Toshiko Hillyard_...|\n",
      "| 28|  Male|     Neda Briski|   OOP| 61973|   69|Alberta Freund_El...|\n",
      "| 28|Female|    Claude Panos| Cloud| 72409|   85|Sheryll Towler_Al...|\n",
      "| 28|  Male|  Celeste Lollis|   MVC| 81492|   64|Nicole Harwood_Cl...|\n",
      "| 29|  Male|  Cordie Harnois|   OOP| 92882|   51|Judie Chipps_Clem...|\n",
      "| 29|Female|       Kena Wild|   DSA|102285|   35|Dustin Feagins_Ma...|\n",
      "| 29|  Male| Ernest Rossbach|    DB|111449|   53|Maybell Duguay_Ab...|\n",
      "| 28|Female|  Latia Vanhoose|    DB|122502|   27|Latia Vanhoose_Mi...|\n",
      "| 29|Female|  Latia Vanhoose|   MVC|132110|   55|Eda Neathery_Nico...|\n",
      "| 29|  Male|     Neda Briski|    PF|141770|   42|Margene Moores_Mi...|\n",
      "| 29|Female|  Latia Vanhoose|    DB|152159|   27|Claude Panos_Sant...|\n",
      "| 29|  Male|  Loris Crossett|   MVC|161771|   36|Mitzi Seldon_Jenn...|\n",
      "| 29|  Male|  Annika Hoffman|   OOP|171660|   22|Taryn Brownlee_Mi...|\n",
      "| 29|  Male|   Santa Kerfien|    PF|182129|   56|Judie Chipps_Tary...|\n",
      "| 28|Female|Mickey Cortright|    DB|192537|   62|Ernest Rossbach_M...|\n",
      "+---+------+----------------+------+------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@15784d84\r\n",
       "df: org.apache.spark.sql.DataFrame = [age: int, gender: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var spark = SparkSession.builder.appName(\"UDF\").getOrCreate()\n",
    " \n",
    "\n",
    "var df = spark.read.option(\"header\",true).option(\"inferSchema\",true).csv(\"Data/StudentData.csv\")\n",
    "df.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6006d934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[age: int, gender: string ... 5 more fields]\n",
      "class org.apache.spark.sql.Column"
     ]
    }
   ],
   "source": [
    "//COMMAND ----------\n",
    " \n",
    "println(df)\n",
    "print(df(\"age\").getClass)\n",
    " \n",
    "//COMMAND ----------\n",
    " \n",
    "//df.collect()\n",
    " \n",
    "// COMMAND ----------\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b324d6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------+------+------+-----+--------------------+\n",
      "|age|gender|            name|course|  roll|marks|               email|\n",
      "+---+------+----------------+------+------+-----+--------------------+\n",
      "| 28|Female| Hubert Oliveras|    DB|  2984|   59|Annika Hoffman_Na...|\n",
      "| 28|  Male|  Celeste Lollis|    PF| 21267|   45|Jeannetta Golden_...|\n",
      "| 28|  Male|  Sheryll Towler|   DSA| 41487|   41|Claude Panos_Judi...|\n",
      "| 28|  Male|  Margene Moores|   MVC| 52771|   32|Toshiko Hillyard_...|\n",
      "| 28|  Male|     Neda Briski|   OOP| 61973|   69|Alberta Freund_El...|\n",
      "| 28|Female|    Claude Panos| Cloud| 72409|   85|Sheryll Towler_Al...|\n",
      "| 28|  Male|  Celeste Lollis|   MVC| 81492|   64|Nicole Harwood_Cl...|\n",
      "| 28|Female|  Latia Vanhoose|    DB|122502|   27|Latia Vanhoose_Mi...|\n",
      "| 28|Female|Mickey Cortright|    DB|192537|   62|Ernest Rossbach_M...|\n",
      "| 28|Female|       Kena Wild| Cloud|221750|   60|Mitzi Seldon_Jenn...|\n",
      "| 28|Female|    Jc Andrepont|   DSA|232060|   58|Billi Clore_Abram...|\n",
      "| 28|Female|  Alberta Freund|   OOP|251805|   83|Annika Hoffman_Sh...|\n",
      "| 28|Female|  Maybell Duguay| Cloud|261439|   20|Nicole Harwood_Ju...|\n",
      "| 28|Female|  Dustin Feagins|   DSA|291984|   82|Abram Nagao_Kena ...|\n",
      "| 28|Female|     Anna Santos|    DB|311589|   79|Celeste Lollis_Mi...|\n",
      "| 28|Female|    Cheri Kenney|   MVC|321816|   24|Kena Wild_Michell...|\n",
      "| 28|Female|  Loris Crossett|    PF|332739|   62|Michelle Ruggiero...|\n",
      "| 28|Female|Mickey Cortright|   DSA|342003|   44|Mitzi Seldon_Jean...|\n",
      "| 28|  Male| Hubert Oliveras|   OOP|351719|   63|Lawanda Wohlwend_...|\n",
      "| 28|  Male| Sebrina Maresca|    PF|361316|   62|Nicole Harwood_La...|\n",
      "+---+------+----------------+------+------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"age==28\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8605715b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class org.apache.spark.rdd.MapPartitionsRDD\n"
     ]
    },
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 13.0 failed 1 times, most recent failure: Lost task 0.0 in stage 13.0 (TID 13, SIddhartha, executor driver): org.apache.spark.SparkException: Dataset transformations and actions can only be invoked by the driver, not inside of other Dataset transformations; for example, dataset1.map(x => dataset2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the dataset1.map transformation. For more information, see SPARK-28702.\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 13.0 failed 1 times, most recent failure: Lost task 0.0 in stage 13.0 (TID 13, SIddhartha, executor driver): org.apache.spark.SparkException: Dataset transformations and actions can only be invoked by the driver, not inside of other Dataset transformations; for example, dataset1.map(x => dataset2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the dataset1.map transformation. For more information, see SPARK-28702.\r",
      "\tat org.apache.spark.sql.Dataset.sparkSession$lzycompute(Dataset.scala:202)\r",
      "\tat org.apache.spark.sql.Dataset.sparkSession(Dataset.scala:196)\r",
      "\tat org.apache.spark.sql.Dataset.sqlContext$lzycompute(Dataset.scala:258)\r",
      "\tat org.apache.spark.sql.Dataset.sqlContext(Dataset.scala:258)\r",
      "\tat org.apache.spark.sql.Dataset.col(Dataset.scala:1350)\r",
      "\tat org.apache.spark.sql.Dataset.apply(Dataset.scala:1320)\r",
      "\tat $anonfun$new$1(<console>:43)\r",
      "\tat $anonfun$new$1$adapted(<console>:43)\r",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\r",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r",
      "\tat scala.collection.AbstractIterator.to(Iterator.scala:1429)\r",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r",
      "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1429)\r",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r",
      "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1429)\r",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)\r",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2154)\r",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r",
      "\tat java.lang.Thread.run(Thread.java:748)\r",
      "\r",
      "Driver stacktrace:\r",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r",
      "  at scala.Option.foreach(Option.scala:407)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2179)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\r",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r",
      "  at org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\r",
      "  ... 37 elided\r",
      "Caused by: org.apache.spark.SparkException: Dataset transformations and actions can only be invoked by the driver, not inside of other Dataset transformations; for example, dataset1.map(x => dataset2.values.count() * x) is invalid because the values transformation and count action cannot be performed inside of the dataset1.map transformation. For more information, see SPARK-28702.\r",
      "  at org.apache.spark.sql.Dataset.sparkSession$lzycompute(Dataset.scala:202)\r",
      "  at org.apache.spark.sql.Dataset.sparkSession(Dataset.scala:196)\r",
      "  at org.apache.spark.sql.Dataset.sqlContext$lzycompute(Dataset.scala:258)\r",
      "  at org.apache.spark.sql.Dataset.sqlContext(Dataset.scala:258)\r",
      "  at org.apache.spark.sql.Dataset.col(Dataset.scala:1350)\r",
      "  at org.apache.spark.sql.Dataset.apply(Dataset.scala:1320)\r",
      "  at $anonfun$new$1(<console>:43)\r",
      "  at $anonfun$new$1$adapted(<console>:43)\r",
      "  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\r",
      "  at scala.collection.Iterator.foreach(Iterator.scala:941)\r",
      "  at scala.collection.Iterator.foreach$(Iterator.scala:941)\r",
      "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r",
      "  at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r",
      "  at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r",
      "  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r",
      "  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r",
      "  at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r",
      "  at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r",
      "  at scala.collection.AbstractIterator.to(Iterator.scala:1429)\r",
      "  at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r",
      "  at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r",
      "  at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1429)\r",
      "  at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r",
      "  at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r",
      "  at scala.collection.AbstractIterator.toArray(Iterator.scala:1429)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1004)\r",
      "  at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2154)\r",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:127)\r",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\r",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\r",
      "  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r",
      "  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r",
      "  ... 1 more\r",
      ""
     ]
    }
   ],
   "source": [
    "// COMMAND ----------\n",
    "val df = spark.read.option(\"header\", true).option(\"inferSchema\", true).csv(\"Data/StudentData.csv\")\n",
    "var rdd = df.rdd\n",
    "println(rdd.getClass)\n",
    "\n",
    "//    val filt = lines.filter(text=>(text.toString.contains(tt => tt in (startdate until enddate))))\n",
    "\n",
    "\n",
    "rdd.filter(age =>(df(\"age\") == 28)).collect()\n",
    "\n",
    " \n",
    "// COMMAND ----------\n",
    " \n",
    "rdd.take(1)\n",
    " \n",
    "// COMMAND ----------\n",
    " \n",
    "//rdd.filter(df(\"age\") == 28).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d8187f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
