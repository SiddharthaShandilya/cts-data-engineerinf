{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841c7aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5230290a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- CRSElapsedTime: string (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiIn: integer (nullable = true)\n",
      " |-- TaxiOut: integer (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: integer (nullable = true)\n",
      " |-- WeatherDelay: integer (nullable = true)\n",
      " |-- NASDelay: integer (nullable = true)\n",
      " |-- SecurityDelay: integer (nullable = true)\n",
      " |-- LateAircraftDelay: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, desc, count\n",
    "spark = SparkSession.builder.appName('trial_mini_project').getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "# importing data from datastet\n",
    "\n",
    "df = spark.read.csv('Data/2007.csv',inferSchema=True,header=True)\n",
    "#df_q2 = df.select('Dest', 'TailNum').where(df[\"Dest\"] != '0')\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88c825b",
   "metadata": {},
   "source": [
    "# 1) Find the most frequent tail number which is getting in destination \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a02facb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      " +++++  +++++  +++++  +++++  +++++  +++++  +++++  +++++  +++++  +++++ \n",
      "+-------+----+-------------+\n",
      "|TailNum|Dest|Count_TailNum|\n",
      "+-------+----+-------------+\n",
      "| N655BR| HNL|         2241|\n",
      "| N651BR| HNL|         2173|\n",
      "| N654BR| HNL|         2138|\n",
      "| N693BR| HNL|         2067|\n",
      "| N479HA| HNL|         2038|\n",
      "| N478HA| HNL|         2024|\n",
      "| N485HA| HNL|         1984|\n",
      "| N480HA| HNL|         1976|\n",
      "| N484HA| HNL|         1944|\n",
      "| N487HA| HNL|         1909|\n",
      "| N481HA| HNL|         1868|\n",
      "| N810AL| HNL|         1843|\n",
      "| N477HA| HNL|         1837|\n",
      "| N837AL| HNL|         1836|\n",
      "| N475HA| HNL|         1816|\n",
      "| N486HA| HNL|         1787|\n",
      "| N646BR| HNL|         1768|\n",
      "| N836AL| HNL|         1761|\n",
      "| N824AL| HNL|         1754|\n",
      "| N808AL| HNL|         1740|\n",
      "+-------+----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, desc,expr\n",
    "\n",
    "#importing data from local data folder\n",
    "df = spark.read.csv('Data/2007.csv',inferSchema=True,header=True)\n",
    "\n",
    "#selecting the required columns , this makes processing faster\n",
    "df_q2 = df.select('Dest','TailNum')\n",
    "\n",
    "df_q2_no_null = df_q2.filter((df_q2[\"TailNum\"] != \"0\") & (df_q2[\"TailNum\"] != \"000000\")) #removing rows with TailNum = 0/000000\n",
    "\n",
    "print(type(df_q2))\n",
    "print(\" +++++ \"*10)\n",
    "most_frequent = df_q2_no_null.groupBy(\"TailNum\",'Dest').agg(count(\"TailNum\").alias(\"Count_TailNum\")).sort(col(\"Count_TailNum\").desc()).show()\n",
    "\n",
    "#most_frequent.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659b8e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "baaa036e",
   "metadata": {},
   "source": [
    "# 2) Find out the cancelled flight  details for the last quarter of the year 2007\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f17928c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|2007|    8|         2|        4|     NA|       830|     NA|       930|           WN|        7|      0|               NA|            60|     NA|      NA|      NA|   DAL| HOU|     239|     0|      0|        1|               A|       0|           0|           0|       0|            0|                0|\n",
      "|2007|    8|         2|        4|     NA|      2000|     NA|      2055|           WN|       56|      0|               NA|            55|     NA|      NA|      NA|   HOU| DAL|     239|     0|      0|        1|               A|       0|           0|           0|       0|            0|                0|\n",
      "|2007|    8|         2|        4|     NA|      1855|     NA|      2035|           WN|      756|      0|               NA|           160|     NA|      NA|      NA|   MCO| MDW|     989|     0|      0|        1|               A|       0|           0|           0|       0|            0|                0|\n",
      "|2007|    8|         2|        4|     NA|       705|     NA|       840|           WN|     1423|      0|               NA|           155|     NA|      NA|      NA|   MCO| MDW|     989|     0|      0|        1|               A|       0|           0|           0|       0|            0|                0|\n",
      "|2007|    8|         2|        4|     NA|      1830|     NA|      2210|           WN|      867|      0|               NA|           160|     NA|      NA|      NA|   MDW| MCO|     989|     0|      0|        1|               A|       0|           0|           0|       0|            0|                0|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('Data/2007.csv',inferSchema=True,header=True)\n",
    "\n",
    "df.na.drop()\n",
    "\n",
    "df.filter((df.Month >= 8) & (df.Month <=12) & (df.Cancelled == 1)).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6af3f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3fb345b",
   "metadata": {},
   "source": [
    "# 3) Find out the average weather delays for a particular flight per month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ff23285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+\n",
      "|Month|  avg(WeatherDelay)|\n",
      "+-----+-------------------+\n",
      "|    1| 0.8126742594025668|\n",
      "|    2| 1.1426651862433788|\n",
      "|    3| 0.6333765638468795|\n",
      "|    4|   0.51643216930666|\n",
      "|    5| 0.6052272846017077|\n",
      "|    6| 1.2763936562420544|\n",
      "|    7| 1.0766004687307265|\n",
      "|    8| 0.8375915956275956|\n",
      "|    9|0.41135346150449775|\n",
      "|   10|0.45674389516057345|\n",
      "|   11| 0.3357768086867862|\n",
      "|   12| 1.1352771929481762|\n",
      "+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, col, desc\n",
    "\n",
    "df = spark.read.csv('Data/2007.csv',inferSchema=True,header=True)\n",
    "\n",
    "df.groupBy(\"Month\").avg('WeatherDelay').sort(col(\"Month\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394d3841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05f751c4",
   "metadata": {},
   "source": [
    "# 4) Inspite of NASDelay, SecurityDelay, LateAircraftDelay,Weatherdealy which flight reached on time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f696d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.csv('Data/2007.csv',inferSchema=True,header=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df.filter(((df.NASDelay > 0) | (df.SecurityDelay > 0) | (df.LateAircraftDelay > 0) | (df.WeatherDelay > 0)) & (df.ArrDelay <= 0) ).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3873d80d",
   "metadata": {},
   "source": [
    "# 5) Month wise total distance travelled by each flight number in every month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fdd15a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+-----+---------+-------------+\n",
      "|Month|FlightNum|sum(Distance)|\n",
      "+-----+---------+-------------+\n",
      "|    1|     2383|        64128|\n",
      "|    1|      285|       394434|\n",
      "|    1|       95|       339349|\n",
      "|    1|      690|       193603|\n",
      "|    1|     1867|        74379|\n",
      "|    1|     2645|        53660|\n",
      "|    1|      534|       221538|\n",
      "|    1|     2016|        81014|\n",
      "|    1|     1528|       156561|\n",
      "|    1|     2057|        19793|\n",
      "|    1|      691|       209452|\n",
      "|    1|     2513|        22538|\n",
      "|    1|      670|       151788|\n",
      "|    1|      463|        63309|\n",
      "|    1|     1423|       164350|\n",
      "|    1|     1871|        91504|\n",
      "|    1|     1705|       124020|\n",
      "|    1|     2017|        51207|\n",
      "|    1|     2565|        11932|\n",
      "|    1|     3253|         3355|\n",
      "+-----+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, desc\n",
    "\n",
    "\n",
    "df = spark.read.csv('Data/2007.csv',inferSchema=True,header=True)\n",
    "df_q2 = df.select('Month','FlightNum', 'Distance').filter((df[\"TailNum\"] != \"0\") & (df[\"TailNum\"] != \"000000\"))\n",
    "\n",
    "print(type(df_q2))\n",
    "\n",
    "total_distance = df_q2.groupBy(\"Month\",\"FlightNum\").sum(\"Distance\").sort(col(\"Month\")).show()\n",
    "\n",
    "#total_count = df_q2.groupBy(\"TailNum\").sum(\"Distance\").sort(col(\"sum(Distance)\").desc()).show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b0b342",
   "metadata": {},
   "source": [
    "# 6) Month wise how many flights get diverted(origin to destination)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10697b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ++++  ++++  ++++  ++++  ++++  ++++  ++++  ++++  ++++  ++++ \n",
      "+-----+-------------+\n",
      "|Month|sum(Diverted)|\n",
      "+-----+-------------+\n",
      "|    1|         1200|\n",
      "|    2|         1261|\n",
      "|    3|         1275|\n",
      "|    4|         1193|\n",
      "|    5|         1442|\n",
      "|    6|         2199|\n",
      "|    7|         2150|\n",
      "|    8|         2100|\n",
      "|    9|          942|\n",
      "|   10|          977|\n",
      "|   11|          845|\n",
      "|   12|         1488|\n",
      "+-----+-------------+\n",
      "\n",
      " ++++  ++++  ++++  ++++  ++++  ++++  ++++  ++++  ++++  ++++ \n",
      "+-----+------+----+-------------+\n",
      "|Month|Origin|Dest|sum(Diverted)|\n",
      "+-----+------+----+-------------+\n",
      "|    1|   ABQ| MCO|            0|\n",
      "|    1|   ABQ| SEA|            0|\n",
      "|    1|   BNA| DTW|            0|\n",
      "|    1|   HOU| AUS|            2|\n",
      "|    1|   SJC| BUR|            0|\n",
      "|    1|   EWR| RDU|            0|\n",
      "|    1|   SHV| IAH|            0|\n",
      "|    1|   EWR| CLT|            1|\n",
      "|    1|   MSY| EWR|            0|\n",
      "|    1|   PHX| ASE|            3|\n",
      "|    1|   CLE| CVG|            0|\n",
      "|    1|   CAK| ATL|            0|\n",
      "+-----+------+----+-------------+\n",
      "only showing top 12 rows\n",
      "\n",
      " ++++  ++++  ++++  ++++  ++++  ++++  ++++  ++++  ++++  ++++ \n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, desc\n",
    "\n",
    "\n",
    "df = spark.read.csv('Data/2007.csv',inferSchema=True,header=True)\n",
    "df_q2 = df.select('Origin','Month','Dest','TailNum','Diverted').filter((df[\"TailNum\"] != \"0\") & (df[\"TailNum\"] != \"000000\"))\n",
    "\n",
    "print(\" ++++ \"*10)\n",
    "\n",
    "# monthwise total diveted flight\n",
    "total_sum_diverted = df_q2.groupBy(\"Month\").sum(\"Diverted\").sort(col(\"Month\")).show()\n",
    "\n",
    "print(\" ++++ \"*10)\n",
    "\n",
    "# monthwise total diveted flight with respect to each origin and destination\n",
    "total_distance = df_q2.groupBy(\"Month\",'Origin','Dest').sum(\"Diverted\").sort(col(\"Month\")).show(12)\n",
    "\n",
    "print(\" ++++ \"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7909260f",
   "metadata": {},
   "source": [
    "# 7) Week and month wise number of trips in all the flights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f58aa2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-------+-----------+\n",
      "|Month|DayOfWeek|TailNum|total_trips|\n",
      "+-----+---------+-------+-----------+\n",
      "|    1|        1| N511UA|         14|\n",
      "|    1|        1| N434YV|         34|\n",
      "|    1|        1| N16170|         21|\n",
      "|    1|        1| N443CA|         21|\n",
      "|    1|        1| N495CA|         29|\n",
      "|    1|        1| N708CA|         22|\n",
      "|    1|        1| N929LR|         30|\n",
      "|    1|        1| N829UA|         19|\n",
      "|    1|        1| N680AW|         23|\n",
      "|    1|        1| N813AW|         24|\n",
      "|    1|        1| N625AW|         22|\n",
      "|    1|        1| N627AW|         13|\n",
      "|    1|        1| N752UW|         26|\n",
      "|    1|        1| N601DL|         15|\n",
      "|    1|        1| N503AE|         28|\n",
      "|    1|        1| N513AE|         25|\n",
      "|    1|        1| N620NW|         17|\n",
      "|    1|        1| N508US|         13|\n",
      "|    1|        1| N583AA|         21|\n",
      "|    1|        1| N958AS|         27|\n",
      "+-----+---------+-------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, desc\n",
    "\n",
    "\n",
    "df = spark.read.csv('Data/2007.csv',inferSchema=True,header=True)\n",
    "df_no_cancel = df.filter('Cancelled== 0') \n",
    "df_q2 = df_no_cancel.select('Month','DayOfWeek','TailNum').filter((df[\"TailNum\"] != \"0\") & (df[\"TailNum\"] != \"000000\") & (df[\"Cancelled\"] == 0))\n",
    "\n",
    "\n",
    "\n",
    "total_distance = df_q2.groupBy(\"Month\",'DayOfWeek','TailNum').agg(count(\"TailNum\").alias(\"total_trips\")).sort(['Month','DayOfWeek']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6778b2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_distance.sort(['Month','DayOfWeek']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bba87f",
   "metadata": {},
   "source": [
    "# 8) Which flights covered maximum origin and destination by month wise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba86827c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      " +++++  +++++  +++++  +++++  +++++  +++++  +++++  +++++  +++++  +++++ \n",
      "+-----+------+----+-------+-----------+\n",
      "|Month|Origin|Dest|TailNum|Total_trips|\n",
      "+-----+------+----+-------+-----------+\n",
      "|    8|   HNL| OGG| N841AL|         94|\n",
      "|   12|   OGG| HNL| N841AL|         86|\n",
      "|    7|   HNL| OGG| N841AL|         86|\n",
      "|   12|   HNL| OGG| N841AL|         86|\n",
      "|    8|   OGG| HNL| N485HA|         85|\n",
      "|    7|   LGA| BOS| N913DE|         85|\n",
      "|   10|   HNL| KOA| N655BR|         84|\n",
      "|   10|   KOA| HNL| N655BR|         84|\n",
      "|    7|   BOS| LGA| N913DE|         83|\n",
      "|    1|   KOA| HNL| N646BR|         83|\n",
      "|    1|   HNL| KOA| N646BR|         82|\n",
      "|   10|   LGA| BOS| N911DE|         82|\n",
      "|   11|   LGA| BOS| N916DE|         82|\n",
      "|    5|   BOS| LGA| N908DE|         81|\n",
      "|    7|   LIH| HNL| N841AL|         80|\n",
      "|   10|   BOS| LGA| N911DE|         79|\n",
      "|    5|   LGA| BOS| N908DE|         79|\n",
      "|    5|   HNL| OGG| N836AL|         79|\n",
      "|    8|   KOA| HNL| N655BR|         79|\n",
      "|   11|   BOS| LGA| N916DE|         78|\n",
      "+-----+------+----+-------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, desc,expr\n",
    "\n",
    "#importing data from local data folder\n",
    "df = spark.read.csv('Data/2007.csv',inferSchema=True,header=True)\n",
    "\n",
    "#selecting the required columns , this makes processing faster\n",
    "df_q2 = df.select('Origin','Month','Dest','TailNum').filter((df[\"TailNum\"] != \"0\") & (df[\"TailNum\"] != \"000000\"))\n",
    "\n",
    "df_q2_no_null = df_q2.filter((df_q2[\"Origin\"] != \"null\") & (df_q2[\"Dest\"] != \"null\")) #removing rows with null values\n",
    "\n",
    "print(type(df_q2))\n",
    "print(\" +++++ \"*10)\n",
    "\n",
    "most_frequent = df_q2_no_null.groupBy('Month','Origin','Dest','TailNum').agg(count(\"TailNum\").alias(\"Total_trips\")).sort(col(\"Total_trips\").desc()).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a410f93a",
   "metadata": {},
   "source": [
    "# 9) Average month wise arrival delay (flightnum wise)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cc44144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-------------------------+\n",
      "|Month|FlightNum|avg_monthly_arrival_delay|\n",
      "+-----+---------+-------------------------+\n",
      "|    1|       59|        6.102941176470588|\n",
      "|    1|      239|        6.021739130434782|\n",
      "|    1|     1972|        6.168539325842697|\n",
      "|    1|       96|       10.427631578947368|\n",
      "|    1|     2543|        5.857142857142857|\n",
      "|    1|     1670|        6.069767441860465|\n",
      "|    1|     1170|        19.62439024390244|\n",
      "|    1|     1839|       2.7282608695652173|\n",
      "|    1|     1703|         9.09727626459144|\n",
      "|    1|      318|        8.606666666666667|\n",
      "|    1|     2592|        1.911764705882353|\n",
      "|    1|     1185|        12.43661971830986|\n",
      "|    1|     1430|       15.057692307692308|\n",
      "|    1|     1799|        6.566666666666666|\n",
      "|    1|     2759|        42.69565217391305|\n",
      "|    1|     3058|       10.226666666666667|\n",
      "|    1|     3014|       1.8641975308641976|\n",
      "|    1|     2898|       0.7391304347826086|\n",
      "|    1|     7109|       16.894736842105264|\n",
      "|    1|     5448|       1.4259259259259258|\n",
      "+-----+---------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, desc\n",
    "\n",
    "\n",
    "df = spark.read.csv('Data/2007.csv',inferSchema=True,header=True)\n",
    "df_no_cancel = df.filter('Cancelled== 0') \n",
    "df_q2 = df_no_cancel.select('Month','ArrDelay','FlightNum')\n",
    "\n",
    "\n",
    "\n",
    "total_distance = df_q2.groupBy(\"Month\", \"FlightNum\").agg(avg(\"ArrDelay\").alias(\"avg_monthly_arrival_delay\")).sort(['Month']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd246c45",
   "metadata": {},
   "source": [
    "# 10) Average month wise departure delay (flightnum wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "665b3e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------------------------+\n",
      "|Month|FlightNum|avg_monthly_departure_delay|\n",
      "+-----+---------+---------------------------+\n",
      "|    1|      739|         3.3737864077669903|\n",
      "|    1|     2344|           7.40952380952381|\n",
      "|    1|     2285|         -4.232558139534884|\n",
      "|    1|     2847|          5.572327044025157|\n",
      "|    1|      547|          17.09205020920502|\n",
      "|    1|     1726|          5.578231292517007|\n",
      "|    1|     2367|         3.8255813953488373|\n",
      "|    1|     2478|       0.033707865168539325|\n",
      "|    1|      847|         12.486842105263158|\n",
      "|    1|      381|         4.7611940298507465|\n",
      "|    1|      152|                  4.8515625|\n",
      "|    1|      541|         12.436681222707424|\n",
      "|    1|     2215|         1.4193548387096775|\n",
      "|    1|     2682|          8.419354838709678|\n",
      "|    1|     2250|         2.6056338028169015|\n",
      "|    1|     1207|          9.425531914893616|\n",
      "|    1|     2267|         31.345454545454544|\n",
      "|    1|     2699|          5.810344827586207|\n",
      "|    1|     7187|                     34.125|\n",
      "|    1|     7174|         13.311475409836065|\n",
      "+-----+---------+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, desc\n",
    "\n",
    "\n",
    "df = spark.read.csv('Data/2007.csv',inferSchema=True,header=True)\n",
    "df_no_cancel = df.filter('Cancelled== 0') \n",
    "df_q2 = df_no_cancel.select('Month','DepDelay','FlightNum')\n",
    "\n",
    "\n",
    "\n",
    "total_distance = df_q2.groupBy(\"Month\",\"FlightNum\").agg(avg(\"DepDelay\").alias(\"avg_monthly_departure_delay\")).sort(['Month']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1a7e32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8515d444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
